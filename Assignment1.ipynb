{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d3ca61",
   "metadata": {},
   "source": [
    "# Assignment 1: Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bec73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569df525",
   "metadata": {},
   "source": [
    "## 1 - Assignment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d915b",
   "metadata": {},
   "source": [
    "## 2 - Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbe730",
   "metadata": {},
   "source": [
    "### 2.1 - The Unicode standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "891d90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr0 = chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51e85bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct print: \"\u0000\"; representation: \"'\\x00'\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Direct print: \"{chr0}\"; representation: \"{chr0.__repr__()}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6e82c",
   "metadata": {},
   "source": [
    "(a) It represents the NULL character\n",
    "\n",
    "(b) Printing it gives nothing, but it's representation is \\x00, which is a bytes literal for 0, which represents U+0000 in Unicode\n",
    "\n",
    "(c) Printing shows nothing, but the representation is still the \\x00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23b392",
   "metadata": {},
   "source": [
    "### 2.2 - Unicode Encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31dad04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hello! こんにちは!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad00e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n"
     ]
    }
   ],
   "source": [
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6be09a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6efb7348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 33,\n",
       " 32,\n",
       " 227,\n",
       " 129,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 227,\n",
       " 129,\n",
       " 171,\n",
       " 227,\n",
       " 129,\n",
       " 161,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 33]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list (utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "744d11dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test string: 13, vs length of utf encoding 23\n"
     ]
    }
   ],
   "source": [
    "print(f'length of test string: {len(test_string)}, vs length of utf encoding {len(utf8_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5171662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 0 utf8 characters: \"\"\n",
      "first 1 utf8 characters: \"h\"\n",
      "first 2 utf8 characters: \"he\"\n",
      "first 3 utf8 characters: \"hel\"\n",
      "first 4 utf8 characters: \"hell\"\n",
      "first 5 utf8 characters: \"hello\"\n",
      "first 6 utf8 characters: \"hello!\"\n",
      "first 7 utf8 characters: \"hello! \"\n",
      "first 8 utf8 characters: <<decoding failed>>\n",
      "first 9 utf8 characters: <<decoding failed>>\n",
      "first 10 utf8 characters: \"hello! こ\"\n",
      "first 11 utf8 characters: <<decoding failed>>\n",
      "first 12 utf8 characters: <<decoding failed>>\n",
      "first 13 utf8 characters: \"hello! こん\"\n",
      "first 14 utf8 characters: <<decoding failed>>\n",
      "first 15 utf8 characters: <<decoding failed>>\n",
      "first 16 utf8 characters: \"hello! こんに\"\n",
      "first 17 utf8 characters: <<decoding failed>>\n",
      "first 18 utf8 characters: <<decoding failed>>\n",
      "first 19 utf8 characters: \"hello! こんにち\"\n",
      "first 20 utf8 characters: <<decoding failed>>\n",
      "first 21 utf8 characters: <<decoding failed>>\n",
      "first 22 utf8 characters: \"hello! こんにちは\"\n"
     ]
    }
   ],
   "source": [
    "for id in range(len(utf8_encoded)):\n",
    "    try:\n",
    "        print(f'first {id} utf8 characters: \"{utf8_encoded[0:id].decode(\"utf-8\")}\"')\n",
    "    except:\n",
    "        print(f'first {id} utf8 characters: <<decoding failed>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1187076b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERROR'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    try:\n",
    "        output_str = \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "    except:\n",
    "        output_str = \"ERROR\"\n",
    "    return output_str\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello!\".encode(\"utf-8\"))\n",
    "decode_utf8_bytes_to_str_wrong(\"hello! こんにちは!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b967003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For character \"こ\", this is the bytes list: [227, 129, 147], representing b'\\xe3\\x81\\x93'\n",
      "If I decode all 3 bytes together I get \"こ\"\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 0-1: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFor character \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mこ\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, this is the bytes list: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mこ\u001b[39m\u001b[33m\"\u001b[39m.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, representing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mこ\u001b[39m\u001b[33m\"\u001b[39m.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf I decode all 3 bytes together I get \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mbytes\u001b[39m(bytes_list).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf I decode only first 2 bytes I get \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbytes_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode bytes in position 0-1: unexpected end of data"
     ]
    }
   ],
   "source": [
    "bytes_list = list(\"こ\".encode(\"utf-8\"))\n",
    "print(f'For character \"こ\", this is the bytes list: {list(\"こ\".encode(\"utf-8\"))}, representing {\"こ\".encode(\"utf-8\")}')\n",
    "print(f'If I decode all 3 bytes together I get \"{bytes(bytes_list).decode(\"utf-8\")}\"')\n",
    "print(f'If I decode only first 2 bytes I get \"{bytes(bytes_list[0:2]).decode(\"utf-8\")}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5c2f0",
   "metadata": {},
   "source": [
    "(a) Reasons to prefer UTF-8 encoded bytes: Bytes are naturally well-aligned with the infrastructure of the internet and codes. UTF-16 and UTF-32 are multi-byte, introducing complexity\n",
    "\n",
    "(b) `decode_utf8_bytes_to_str_wrong` doesn't work for characters that are longer than 1 byte.\n",
    "\n",
    "(c) `\\xe3\\x81`, which is only the first 2 of 3 bytes in こ (ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e6c14",
   "metadata": {},
   "source": [
    "### 2.3 - Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fd516",
   "metadata": {},
   "source": [
    "### 2.4 - BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f3e4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_tokenize = \"Some text that I'll pre-tokenize. こんにちは!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef18e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some',\n",
       " ' text',\n",
       " ' that',\n",
       " ' I',\n",
       " \"'ll\",\n",
       " ' pre',\n",
       " '-',\n",
       " 'tokenize',\n",
       " '.',\n",
       " ' こんにちは',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "re.findall(PAT, text_to_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27fe4f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Some',\n",
       " b' text',\n",
       " b' that',\n",
       " b' I',\n",
       " b\"'ll\",\n",
       " b' pre',\n",
       " b'-',\n",
       " b'tokenize',\n",
       " b'.',\n",
       " b' \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT_BYTE = rb\"'(?:[sdmt]|ll|ve|re)| ?[A-Za-z]+| ?[0-9]+| ?[^\\sA-Za-z0-9]+|\\s+(?!\\S)|\\s+\"\n",
    "\n",
    "re.findall(PAT_BYTE, text_to_tokenize.encode(\"utf_8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a34d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '!', ' こんにちは', '!']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PAT,\"hello! こんにちは!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bfa48",
   "metadata": {},
   "source": [
    "Algorithm 1 of Sennrich et al. [2016]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "307df479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i],symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c725ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m num_merges = \u001b[32m10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_merges):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     pairs = \u001b[43mget_stats\u001b[49m(vocab)\n\u001b[32m      8\u001b[39m     best = \u001b[38;5;28mmax\u001b[39m(pairs, key=pairs.get)\n\u001b[32m      9\u001b[39m     vocab = merge_vocab(best, vocab)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_stats' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f'iteration {i}: best match \"{best}\"')\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864cb77",
   "metadata": {},
   "source": [
    "Relation between `num_merges` and `vocab` size:\n",
    "\n",
    "- If you run a low number of merges, you're ending up with almost the same encoding as UTF-8\n",
    "\n",
    "- The more you run the, closer you get to full word encoding, where each word is represented directly by a token.\n",
    "\n",
    "- However, if you introduce new words, they will obviously still be encoded using existing pieces, that are smaller than the word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a30ace",
   "metadata": {},
   "source": [
    "### 2.5 - Experimenting with BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ce4257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file size: 289998753\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/owt_valid.txt\", \"rb\") as file:    \n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    print(f'file size: {file_size}')\n",
    "    file.seek(0)\n",
    "\n",
    "    desired_num_chunks = 3\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f2856667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6896.00s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289998753\n"
     ]
    }
   ],
   "source": [
    "!stat -f %z data/owt_valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a49f84",
   "metadata": {},
   "source": [
    "Diary on profiling\n",
    "\n",
    "1. I ran profiling on TinyStoriesV2-GPT4-train.txt and it gave me the entries for profiles/profile_with_pretokenization. Time spent is roughly split by build_pretokens, get_stats and apply_merge\n",
    "2. We can speed up build_pretokens by running it in parallel\n",
    "3. We can speed up get_stats and apply_merge by using a better algorithm as explained at the bottom of page 8 in [cs336_spring2025_assignment1_basics.pdf](cs336_spring2025_assignment1_basics.pdf)\n",
    "4. We should focus first on improving the algorithm\n",
    "5. I implemented a completely new algorithm that uses minheaps that contain the occurences of affected pairs. As you can see in `output_owt_train_20251104_212059.log` this gave a total execution time of 13 minutes for computing pretokens but about 4 hours for the merging (21:37-01:34)\n",
    "6. I then ran this through Scalene and it seems that most gains were to be made in a single line of code: `negc, _seq_left, _seq_right, stamp, pair = heapq.heappop(heap)`.\n",
    "7. It turns out that the comparison logic is probably expensive, as it is not done in the native C implementation of `heapq` but in a custom Python class `class _DescendingSeq` which I introduced to ensure lexicographical maximum ordering. This could perhaps save up to 19% of total execution time, so we could explore this to shave off something like 45 minutes perhaps.\n",
    "8. I decided to only remove `isinstance` checks, and not focus on the rest.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503929b",
   "metadata": {},
   "source": [
    "BPE on TinyStories:\n",
    "\n",
    "- Use `output_TinyStoriesV2-GPT4-train_20251104_211758.log` for reference\n",
    "- pretokenization takes 0:02:37 (21:17:58-21:20:35)\n",
    "- merging takes very little time 0:00:08 (21:20:36-21:20:44)\n",
    "- total takes under 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6d78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb3672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d838c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d1fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford-cs336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
