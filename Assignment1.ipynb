{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d3ca61",
   "metadata": {},
   "source": [
    "# Assignment 1: Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bec73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569df525",
   "metadata": {},
   "source": [
    "## 1 - Assignment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d915b",
   "metadata": {},
   "source": [
    "## 2 - Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbe730",
   "metadata": {},
   "source": [
    "### 2.1 - The Unicode standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "891d90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr0 = chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51e85bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct print: \"\u0000\"; representation: \"'\\x00'\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Direct print: \"{chr0}\"; representation: \"{chr0.__repr__()}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6e82c",
   "metadata": {},
   "source": [
    "(a) It represents the NULL character\n",
    "\n",
    "(b) Printing it gives nothing, but it's representation is \\x00, which is a bytes literal for 0, which represents U+0000 in Unicode\n",
    "\n",
    "(c) Printing shows nothing, but the representation is still the \\x00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23b392",
   "metadata": {},
   "source": [
    "### 2.2 - Unicode Encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31dad04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hello! こんにちは!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad00e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n"
     ]
    }
   ],
   "source": [
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6be09a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6efb7348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 33,\n",
       " 32,\n",
       " 227,\n",
       " 129,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 227,\n",
       " 129,\n",
       " 171,\n",
       " 227,\n",
       " 129,\n",
       " 161,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 33]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list (utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "744d11dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test string: 13, vs length of utf encoding 23\n"
     ]
    }
   ],
   "source": [
    "print(f'length of test string: {len(test_string)}, vs length of utf encoding {len(utf8_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5171662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 0 utf8 characters: \"\"\n",
      "first 1 utf8 characters: \"h\"\n",
      "first 2 utf8 characters: \"he\"\n",
      "first 3 utf8 characters: \"hel\"\n",
      "first 4 utf8 characters: \"hell\"\n",
      "first 5 utf8 characters: \"hello\"\n",
      "first 6 utf8 characters: \"hello!\"\n",
      "first 7 utf8 characters: \"hello! \"\n",
      "first 8 utf8 characters: <<decoding failed>>\n",
      "first 9 utf8 characters: <<decoding failed>>\n",
      "first 10 utf8 characters: \"hello! こ\"\n",
      "first 11 utf8 characters: <<decoding failed>>\n",
      "first 12 utf8 characters: <<decoding failed>>\n",
      "first 13 utf8 characters: \"hello! こん\"\n",
      "first 14 utf8 characters: <<decoding failed>>\n",
      "first 15 utf8 characters: <<decoding failed>>\n",
      "first 16 utf8 characters: \"hello! こんに\"\n",
      "first 17 utf8 characters: <<decoding failed>>\n",
      "first 18 utf8 characters: <<decoding failed>>\n",
      "first 19 utf8 characters: \"hello! こんにち\"\n",
      "first 20 utf8 characters: <<decoding failed>>\n",
      "first 21 utf8 characters: <<decoding failed>>\n",
      "first 22 utf8 characters: \"hello! こんにちは\"\n"
     ]
    }
   ],
   "source": [
    "for id in range(len(utf8_encoded)):\n",
    "    try:\n",
    "        print(f'first {id} utf8 characters: \"{utf8_encoded[0:id].decode(\"utf-8\")}\"')\n",
    "    except:\n",
    "        print(f'first {id} utf8 characters: <<decoding failed>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1187076b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERROR'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    try:\n",
    "        output_str = \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "    except:\n",
    "        output_str = \"ERROR\"\n",
    "    return output_str\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello!\".encode(\"utf-8\"))\n",
    "decode_utf8_bytes_to_str_wrong(\"hello! こんにちは!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b967003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For character \"こ\", this is the bytes list: [227, 129, 147], representing b'\\xe3\\x81\\x93'\n",
      "If I decode all 3 bytes together I get \"こ\"\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 0-1: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFor character \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mこ\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, this is the bytes list: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mこ\u001b[39m\u001b[33m\"\u001b[39m.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, representing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mこ\u001b[39m\u001b[33m\"\u001b[39m.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf I decode all 3 bytes together I get \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mbytes\u001b[39m(bytes_list).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf I decode only first 2 bytes I get \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbytes_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode bytes in position 0-1: unexpected end of data"
     ]
    }
   ],
   "source": [
    "bytes_list = list(\"こ\".encode(\"utf-8\"))\n",
    "print(f'For character \"こ\", this is the bytes list: {list(\"こ\".encode(\"utf-8\"))}, representing {\"こ\".encode(\"utf-8\")}')\n",
    "print(f'If I decode all 3 bytes together I get \"{bytes(bytes_list).decode(\"utf-8\")}\"')\n",
    "print(f'If I decode only first 2 bytes I get \"{bytes(bytes_list[0:2]).decode(\"utf-8\")}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5c2f0",
   "metadata": {},
   "source": [
    "(a) Reasons to prefer UTF-8 encoded bytes: Bytes are naturally well-aligned with the infrastructure of the internet and codes. UTF-16 and UTF-32 are multi-byte, introducing complexity\n",
    "\n",
    "(b) `decode_utf8_bytes_to_str_wrong` doesn't work for characters that are longer than 1 byte.\n",
    "\n",
    "(c) `\\xe3\\x81`, which is only the first 2 of 3 bytes in こ (ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e6c14",
   "metadata": {},
   "source": [
    "### 2.3 - Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fd516",
   "metadata": {},
   "source": [
    "### 2.4 - BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f3e4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_tokenize = \"Some text that I'll pre-tokenize. こんにちは!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef18e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some',\n",
       " ' text',\n",
       " ' that',\n",
       " ' I',\n",
       " \"'ll\",\n",
       " ' pre',\n",
       " '-',\n",
       " 'tokenize',\n",
       " '.',\n",
       " ' こんにちは',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "re.findall(PAT, text_to_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27fe4f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Some',\n",
       " b' text',\n",
       " b' that',\n",
       " b' I',\n",
       " b\"'ll\",\n",
       " b' pre',\n",
       " b'-',\n",
       " b'tokenize',\n",
       " b'.',\n",
       " b' \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT_BYTE = rb\"'(?:[sdmt]|ll|ve|re)| ?[A-Za-z]+| ?[0-9]+| ?[^\\sA-Za-z0-9]+|\\s+(?!\\S)|\\s+\"\n",
    "\n",
    "re.findall(PAT_BYTE, text_to_tokenize.encode(\"utf_8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a34d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '!', ' こんにちは', '!']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PAT,\"hello! こんにちは!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bfa48",
   "metadata": {},
   "source": [
    "Algorithm 1 of Sennrich et al. [2016]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "307df479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i],symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c725ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m num_merges = \u001b[32m10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_merges):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     pairs = \u001b[43mget_stats\u001b[49m(vocab)\n\u001b[32m      8\u001b[39m     best = \u001b[38;5;28mmax\u001b[39m(pairs, key=pairs.get)\n\u001b[32m      9\u001b[39m     vocab = merge_vocab(best, vocab)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_stats' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f'iteration {i}: best match \"{best}\"')\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864cb77",
   "metadata": {},
   "source": [
    "Relation between `num_merges` and `vocab` size:\n",
    "\n",
    "- If you run a low number of merges, you're ending up with almost the same encoding as UTF-8\n",
    "\n",
    "- The more you run the, closer you get to full word encoding, where each word is represented directly by a token.\n",
    "\n",
    "- However, if you introduce new words, they will obviously still be encoded using existing pieces, that are smaller than the word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a30ace",
   "metadata": {},
   "source": [
    "### 2.5 - Experimenting with BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ce4257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file size: 289998753\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/owt_valid.txt\", \"rb\") as file:    \n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    print(f'file size: {file_size}')\n",
    "    file.seek(0)\n",
    "\n",
    "    desired_num_chunks = 3\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f2856667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6896.00s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289998753\n"
     ]
    }
   ],
   "source": [
    "!stat -f %z data/owt_valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a49f84",
   "metadata": {},
   "source": [
    "Diary on profiling\n",
    "\n",
    "1. I ran profiling on TinyStoriesV2-GPT4-train.txt and it gave me the entries for profiles/profile_with_pretokenization. Time spent is roughly split by build_pretokens, get_stats and apply_merge\n",
    "2. We can speed up build_pretokens by running it in parallel\n",
    "3. We can speed up get_stats and apply_merge by using a better algorithm as explained at the bottom of page 8 in [cs336_spring2025_assignment1_basics.pdf](cs336_spring2025_assignment1_basics.pdf)\n",
    "4. We should focus first on improving the algorithm\n",
    "5. I implemented a completely new algorithm that uses minheaps that contain the occurences of affected pairs. As you can see in `output_owt_train_20251104_212059.log` this gave a total execution time of 13 minutes for computing pretokens but about 4 hours for the merging (21:37-01:34)\n",
    "6. I then ran this through Scalene and it seems that most gains were to be made in a single line of code: `negc, _seq_left, _seq_right, stamp, pair = heapq.heappop(heap)`.\n",
    "7. It turns out that the comparison logic is probably expensive, as it is not done in the native C implementation of `heapq` but in a custom Python class `class _DescendingSeq` which I introduced to ensure lexicographical maximum ordering. This could perhaps save up to 19% of total execution time, so we could explore this to shave off something like 45 minutes perhaps.\n",
    "8. I decided to only remove `isinstance` checks, and not focus on the rest.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503929b",
   "metadata": {},
   "source": [
    "BPE on TinyStories:\n",
    "\n",
    "(a) Overview \n",
    "- Use `output_TinyStoriesV2-GPT4-train_20251104_211758.log` for reference\n",
    "- pretokenization takes 0:02:37 (21:17:58-21:20:35)\n",
    "- merging takes very little time 0:00:08 (21:20:36-21:20:44)\n",
    "- total takes under 3 minutes\n",
    "\n",
    "(b) Profile\n",
    "- Use `profiles/profile_TinyStoriesV2-GPT4-train_20251105_092150.html` \n",
    "- Note that most optimizable time (indicated with `python`) is spent in `negc, _seq_left, _seq_right, stamp, pair = heapq.heappop(heap)`\n",
    "- However, the reality is that this is the `_DescendingSeq` custom class that is used for sorting, as discussed above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da81884",
   "metadata": {},
   "source": [
    "BPE on Open Web Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OWT vocab (32,000 entries) from output_owt_train_20251105_204523_vocab.pkl\n",
      "Loaded TinyStories vocab (10,000 entries) from output_TinyStoriesV2-GPT4-train_20251105_092150_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def load_latest_vocab(pattern: str):\n",
    "    matches = sorted(Path(\"outputs\").glob(pattern))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No files matched pattern: {pattern}\")\n",
    "    vocab_path = matches[-1]\n",
    "    with open(vocab_path, \"rb\") as fh:\n",
    "        vocab = pickle.load(fh)\n",
    "    return vocab_path, vocab\n",
    "\n",
    "owt_path, owt_vocab = load_latest_vocab(\"output_owt_train_*_vocab.pkl\")\n",
    "ts_path, ts_vocab = load_latest_vocab(\"output_TinyStoriesV2-GPT4-train_*_vocab.pkl\")\n",
    "\n",
    "print(f\"Loaded OWT vocab ({len(owt_vocab):,} entries) from {owt_path.name}\")\n",
    "print(f\"Loaded TinyStories vocab ({len(ts_vocab):,} entries) from {ts_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad29725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 25810, Length: 64, Value: b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82'\n",
      "Key: 25824, Length: 64, Value: b'----------------------------------------------------------------'\n",
      "Key: 31257, Length: 48, Value: b'\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94'\n",
      "Key: 10903, Length: 32, Value: b'--------------------------------'\n",
      "Key: 15944, Length: 32, Value: b'________________________________'\n",
      "Key: 16882, Length: 32, Value: b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82'\n",
      "Key: 25134, Length: 32, Value: b'================================'\n",
      "Key: 28569, Length: 32, Value: b'................................'\n",
      "Key: 31145, Length: 32, Value: b'********************************'\n",
      "Key: 15278, Length: 24, Value: b'\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94'\n",
      "Key: 23318, Length: 19, Value: b' disproportionately'\n",
      "Key: 24257, Length: 19, Value: b' telecommunications'\n",
      "Key: 260, Length: 18, Value: b'<|file_separator|>'\n",
      "Key: 28259, Length: 18, Value: b' environmentalists'\n",
      "Key: 14284, Length: 17, Value: b' responsibilities'\n",
      "Key: 16281, Length: 17, Value: b' unconstitutional'\n",
      "Key: 25686, Length: 17, Value: b' cryptocurrencies'\n",
      "Key: 26061, Length: 17, Value: b' disproportionate'\n",
      "Key: 27022, Length: 17, Value: b' misunderstanding'\n",
      "Key: 28477, Length: 17, Value: b' counterterrorism'\n"
     ]
    }
   ],
   "source": [
    "# Get the 5 keys with the longest values in owt_vocab\n",
    "longest_items = sorted(owt_vocab.items(), key=lambda kv: len(kv[1]), reverse=True)[:20]\n",
    "for k, v in longest_items:\n",
    "    print(f\"Key: {k}, Length: {len(v)}, Value: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97d82d",
   "metadata": {},
   "source": [
    "\n",
    "(a) Overview\n",
    "* Use `outputs/output_owt_train_20251105_204523.log` for reference, though I turned off debug so the amount of information is less\n",
    "* For profile please see `profiles/profile_owt_train_20251105_204523.html` which shows interestingly that the situation is better than for TinyStories: Proportionally less time is spent on `python` (18% only).\n",
    "* The computation does, however, halt at various times for an extended amount of times (~minutes easily), which I think might be due to python memory cleanup or something related. That would ipmly that this system would run faster on a less resource-constrained system.\n",
    "* Currently the maximum memory footprint is about 24GB, and the total runtime is under 2h. Note that the log above takes a precreated pretoken list and saves off about 20 minutes leading to a merge time of 1:32:05 (20:45:27-22:17:32).\n",
    "* The longest 20 tokens contain some that just have high enough frequency, but also a file separator, garbage and other stuff. This is how chatGPT classifies them:\n",
    "\n",
    "| Key(s) | Token Value (decoded) | Category | Explanation |\n",
    "|---------|-----------------------|-----------|--------------|\n",
    "| 25810, 16882 | ÃÂÃÂÃÂÃÂ... | **1. Encoding noise** | Double-encoded UTF-8 garbage from misinterpreted Unicode (“ÃÂ” artifacts). |\n",
    "| 25824, 10903, 15944, 25134, 28569, 31145 | ----------------, --------------------------------, ________________________________, ================================, ................................, ******************************** | **2. Visual separators (ASCII)** | Markdown or ASCII-art dividers made of repeated punctuation. |\n",
    "| 31257, 15278 | ————————————————, ———————— | **3. Unicode punctuation** | Repeated em dashes (`\\xe2\\x80\\x94`) used as stylistic or section breaks. |\n",
    "| 23318, 24257, 28259, 14284, 16281, 25686, 26061, 27022, 28477 | disproportionately, telecommunications, environmentalists, responsibilities, unconstitutional, cryptocurrencies, disproportionate, misunderstanding, counterterrorism | **4. Common long words** | Whole high-frequency English words learned as single tokens. |\n",
    "| 260 | <\\|file_separator\\|> | **5. Special marker** | Control token marking file boundaries during dataset creation. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15cf5cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWT-only: 24,678 tokens (77.1188% of its vocab)\n",
      "  sample:\n",
      "   b'----------------------------------------------------------------'\n",
      "   b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82'\n",
      "   b'\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94'\n",
      "   b'********************************'\n",
      "   b'--------------------------------'\n",
      "   b'................................'\n",
      "   b'================================'\n",
      "   b'________________________________'\n",
      "   b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82'\n",
      "   b'\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94\\xe2\\x80\\x94'\n",
      "   b' disproportionately'\n",
      "   b' telecommunications'\n",
      "   b' environmentalists'\n",
      "   b' characterization'\n",
      "   b' counterterrorism'\n",
      "   b' cryptocurrencies'\n",
      "   b' disproportionate'\n",
      "   b' misunderstanding'\n",
      "   b' responsibilities'\n",
      "   b' unconstitutional'\n",
      "   b' Charlottesville'\n",
      "   b' Representatives'\n",
      "   b' accomplishments'\n",
      "   b' administrations'\n",
      "   b' characteristics'\n",
      "TinyStories-only: 2,678 tokens (26.7800% of its vocab)\n",
      "  sample:\n",
      "   b' congratulated'\n",
      "   b' granddaughter'\n",
      "   b' caterpillars'\n",
      "   b' imaginations'\n",
      "   b' marshmallows'\n",
      "   b' strawberries'\n",
      "   b' veterinarian'\n",
      "   b' Immediately'\n",
      "   b' adventurous'\n",
      "   b' automobiles'\n",
      "   b' blueberries'\n",
      "   b' butterflies'\n",
      "   b' caterpillar'\n",
      "   b' cauliflower'\n",
      "   b' decorations'\n",
      "   b' disagreeing'\n",
      "   b' firefighter'\n",
      "   b' grandparent'\n",
      "   b' grasshopper'\n",
      "   b' hairdresser'\n",
      "   b' heartbroken'\n",
      "   b' invitations'\n",
      "   b' marshmallow'\n",
      "   b' mischievous'\n",
      "   b' motorcycles'\n",
      "Shared tokens: 7,322 (covers 73.2200% of TinyStories)\n"
     ]
    }
   ],
   "source": [
    "def vocab_values(vocab_dict):\n",
    "    return set(vocab_dict.values())\n",
    "\n",
    "owt_tokens = vocab_values(owt_vocab)\n",
    "ts_tokens = vocab_values(ts_vocab)\n",
    "\n",
    "owt_only_tokens = owt_tokens - ts_tokens\n",
    "ts_only_tokens = ts_tokens - owt_tokens\n",
    "overlap_tokens = owt_tokens & ts_tokens\n",
    "\n",
    "def describe(label, diff_set, base_size, show_sample=25):\n",
    "    percentage = (len(diff_set) / base_size * 100) if base_size else 0.0\n",
    "    print(f\"{label}: {len(diff_set):,} tokens ({percentage:.4f}% of its vocab)\")\n",
    "    if diff_set:\n",
    "        preview = sorted(diff_set, key=lambda tok: (-len(tok), tok))[:show_sample]\n",
    "        print(\"  sample:\")\n",
    "        for tok in preview:\n",
    "            print(f\"   {tok}\")\n",
    "\n",
    "describe(\"OWT-only\", owt_only_tokens, len(owt_tokens))\n",
    "describe(\"TinyStories-only\", ts_only_tokens, len(ts_tokens))\n",
    "print(f\"Shared tokens: {len(overlap_tokens):,} (covers {len(overlap_tokens) / len(ts_tokens) * 100:.4f}% of TinyStories)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f1bd3",
   "metadata": {},
   "source": [
    "(b) Comparison and contrast between OWT and TinyStories:\n",
    "\n",
    "* OWT Contains a lot more decoding artifacts and coding artifacts\n",
    "* TinyStories contain more long words\n",
    "* These differences could be due to the different classes of documents int he corpora, which seem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af22f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford-cs336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
